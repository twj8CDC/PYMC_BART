{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf45957-c32a-4dc7-b442-80967288c81f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Objective\n",
    "- Demonstrate the effect of downsampling and matched cohorts have on outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827fd8f7-fe0e-4b2d-9793-87cedd05b1f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install scikit-survival\n",
    "# %pip install pymc_experimental\n",
    "# %pip install matplotlib colorcet\n",
    "# %pip install --force-reinstall pymc\n",
    "%pip install pymc_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b54f8d-a02d-457d-a937-c2fe4eccb7eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "# import mlflow as ml\n",
    "\n",
    "import utillities as ut\n",
    "import surv_bart as bmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c86085e-7e7d-41da-a036-d6e022e2ac2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are two connected py files for this workflow. \n",
    "- Utilities provides the simulation generator fx and additional evaluation metrics\n",
    "- surv_bart (bmb) is the bart model wrapper. With an sklearn like api it conveniently extends the bart model for sv settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9aa7c96-5fbd-4d2a-a822-2535a08e7299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(ut)\n",
    "importlib.reload(bmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f360f9db-f9a6-453c-984e-4e863758f385",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create a simulation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f258e0-fd4e-4970-9c06-cfac000efb5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generage the random number generator object\n",
    "rng = np.random.default_rng(seed=990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b28bb3b-b82b-4a25-b107-8aecd302877b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Simulation is based on a weibull model with a lambda and alpha parameters. It generates true survival/hazard fxs as well as random event/status times from the inverse of the weibull pdf. \n",
    "\n",
    "The covariate matrix is generated from as number of observations (N), number of variables (x_vars) and class of variables beings binary or continuous. Probability of draws for the var_class ==2 (binary) is given by the corresponding index of the var_prob. \n",
    "\n",
    "Lambda and alpha are calculated from the linear equation defined around the x_mat covariate matrix. \n",
    "\n",
    "eos and time_scale represent an end of study marker and a event time scaling to reduce time points.\n",
    "\n",
    "Returns:\n",
    "- t_event is event time\n",
    "- status is 1 if event occurs or 0 if censored\n",
    "- x_mat is the covariate matrix\n",
    "- true is the true sv/hz at times\n",
    "- true_scale is the true sv/hz at times scaled to the t_event scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a4c35e-dd92-4447-8a43-d172bfc031a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "X_VARS = 10\n",
    "VAR_CLASS = [2,10,3,1,2]\n",
    "VAR_PROB = [0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "LAMBDA = \"np.exp(-5 + .2*x_mat[:,0] + 0.01*np.log(x_mat[:,1]+0.00001) + 0.2*(x_mat[:,2] + x_mat[:,3] + x_mat[:,4]))\" \n",
    "ALPHA_F = \"3 + .1*x_mat[:,0]\"\n",
    "eos = 120\n",
    "time_scale=20\n",
    "\n",
    "\n",
    "# get SV\n",
    "x_mat = ut.get_x_matrix(N=N, x_vars=X_VARS, VAR_CLASS=VAR_CLASS, VAR_PROB=VAR_PROB, rng=rng)\n",
    "t_event, status, x_mat, true, true_scale = ut.sim_surv(x_mat, LAMBDA, ALPHA_F, eos, time_scale=time_scale, return_full=True, rng = rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38cb8f90-0fee-41f2-a69f-35b3a8a476c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A test dataset can be created using the simulation fxs with the same parameters as the train dataset. The rng object instantiated above allows for reproducible sampling with a set seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f56710-3d61-45b3-9f34-586c538091d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "t_x_mat = ut.get_x_matrix(N=N, x_vars=X_VARS, VAR_CLASS=VAR_CLASS, VAR_PROB=VAR_PROB, rng=rng)\n",
    "t_t_event, t_status, t_x_mat, t_true, t_true_scale = ut.sim_surv(t_x_mat, LAMBDA, ALPHA_F, eos, time_scale=time_scale, return_full=True, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2198bc43-309a-4a2a-acf9-c34ec0d88ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To check the the simulated datasets are appropriate comparison of the simulated KPM output scaled and uncaled to the true sv times is completed below. \n",
    "\n",
    "With a high number of observations it is apparent that the simulated datasets KPM stratified on the first covariate (which has been named \"covid\") for correspondence with the actual proposed analyses appropriately approximates the true sv curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e77d1c6-adb9-4561-aba5-aed04d763193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ut.quick_kpm_true(x_mat, status, t_event, true, true_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ebb404-ce24-4f39-8b19-ac5f4b9319c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Similarily the test set evaluation is completed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "017f28e2-3db8-4210-be40-1fb617ad20bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test comparison\n",
    "ut.quick_kpm_true(t_x_mat, t_status, t_t_event, t_true, t_true_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de6eecbe-1300-47b3-a263-bbca38b16248",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally the comparison of the scaled times and the true scaled times are completed below. Demonstrating that scaling the event times does not disrupt the estimator or properties of the sv curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964e80a2-59e0-40f8-ac49-c0fcae11b648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ut.quick_kpm_true_scale(x_mat, status, t_event, true_scale, time_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb43b0f6-20ff-42b2-98d2-8bd2e0a55463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "ut.quick_kpm_true_scale(t_x_mat, t_status, t_t_event, t_true_scale, time_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c43d8d9-2371-4933-b4fc-4e07c035e705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The proposed work is focused on evaluating a main variable (\"covid\") and this corresponding simulation follows this example w/ the first covariate being a binary variable representing covid status. \n",
    "\n",
    "Simple frequencies of the distributions of covid status and corresponding event status are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e0d92d-49ef-4816-b015-86034bf164bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mc_mask = (x_mat[:,0] == 1)\n",
    "train_dict = {\n",
    "    \"N\":status.shape,\n",
    "    \"minor class cases\" :status[mc_mask].shape[0],\n",
    "    \"minor class events\": status[mc_mask].sum(),\n",
    "    \"minor class ev prop\": status[mc_mask].sum()/status[mc_mask].shape[0],\n",
    "    \"major class cases\":status[~mc_mask].shape[0],\n",
    "    \"major class events\" : status[~mc_mask].sum(),\n",
    "    \"major class ev prop\": status[~mc_mask].sum()/status[~mc_mask].shape[0]\n",
    "}\n",
    "\n",
    "mc_mask = (t_x_mat[:,0] == 1)\n",
    "test_dict = {\n",
    "    \"N\":status.shape,\n",
    "    \"minor class cases\" :t_status[mc_mask].shape[0],\n",
    "    \"minor class events\": t_status[mc_mask].sum(),\n",
    "    \"minor class ev prop\": t_status[mc_mask].sum()/t_status[mc_mask].shape[0],\n",
    "    \"major class cases\":t_status[~mc_mask].shape[0],\n",
    "    \"major class events\" : t_status[~mc_mask].sum(),\n",
    "    \"major class ev prop\": t_status[~mc_mask].sum()/t_status[~mc_mask].shape[0]\n",
    "}\n",
    "\n",
    "# log these\n",
    "print(train_dict)\n",
    "print(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7fc7881-ec25-4447-90bc-1471d334b61e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Surv Bart Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40c9ad9b-c7f6-4da4-aa54-3787c054fc8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Surv Bart model is self contained in the surv_bart.py script and access to the prior/model/posteriro fx are accessible as bmb.fx.\n",
    "\n",
    "The first step is to complete simple transformation of the event_time/status datasets to achieve a long-form sv dataset that can be passed to the bart model. \n",
    "\n",
    "Steps:\n",
    "1. get_time_transform - scales the event times to coarsen the data\n",
    "    - this allows for more efficient computation as the long-form data entered into the model is of a length n*k were k is the number of distinct time points.\n",
    "2. get_y_sklearn - joins the status and event_times together into a array of tuples that follow the schema for the scikit_survival package.\n",
    "3. get_case_cohort - used only in the case of class imbalanced cases (<10%). This returned dataset is a case-cohort sample.\n",
    "    - when prop = 1, it returns an identity matrix with the y_sk, x_mat and weights of 1.\n",
    "4. surv_pre_train - creates the long-form training matrix. coh_x now contains the extended time in the first column.\n",
    "5. get_posterior_test - generates a longform test dataset for predictions through the full length of the survival dataset. \n",
    "    - Note that in this first case it is generated on the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60000f54-5860-4e2a-b913-54071949b24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t_event2 = bmb.get_time_transform(t_event, time_scale=time_scale)\n",
    "y_sk = bmb.get_y_sklearn(status, t_event2)\n",
    "y_sk_coh, x_sk_coh, w_coh= bmb.get_case_cohort(y_sk, x_mat, prop = 1)\n",
    "coh_y, coh_x, coh_w, coh_coords = bmb.surv_pre_train(y_sk_coh, x_sk_coh, w_coh)\n",
    "x_tst, tst_coords = bmb.get_posterior_test(np.unique(y_sk_coh[\"Survival_in_days\"]), x_sk_coh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3626a29-175b-4b03-9264-391103e3449f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below the bart model is initiated. There are two parameter objects that needs to be created; the model_dict and sampler_dict.\n",
    "- model_dict \n",
    "    1. trees is the number of trees to generate (20-100 seems to be appropriate). Greater number of trees slows down the analyses.\n",
    "    2. split_rules is a list of split rules corresponding to each variable in the covariate matrix. The first split rule should always be ContinuousSplitRule since it corresponds to the time column of the long-form x_mat. OneHotSplitRule is used for categorical and binary data, while Continuous should be used for all other variables.\n",
    "    3. Split prior is a new feature I am working out. It provides the probability of the feature to be selected for a tree. By default this probability is plit uniformly across all variables, but intuition suggests that having a greater probability on the time variable allows for fewer trees as the time is always going to be the major predictor we want included. The split_prior component can be removed if unwanted.\n",
    "- sampler_dict contains parameters specific for the the sampling step of MCMC. These can all be adjusted. Ideally the number of draws and tune steps should be a balance of sufficiency and computational time. The BART algorithm seems to converge to the sampling distribution quickly, so a smaller number of tune and draws could be beneficial for sampling time.\n",
    "\n",
    "The bart class object is instantiated with the model parameters. Internally is constructs the framework for the bart model that can be used in a fit step completed after it is instantiated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e910e07a-7741-4977-8bed-20a7d9ada235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# intitialize models\n",
    "model_dict = {\"trees\": 20,\n",
    "    \"split_rules\": [\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.OneHotSplitRule\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.OneHotSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\",\n",
    "    \"pmb.ContinuousSplitRule()\"\n",
    "    ],\n",
    "    \"split_prior\": [0.9,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002,\n",
    "                0.002]\n",
    "}\n",
    "\n",
    "sampler_dict = {\n",
    "            \"draws\": 100,\n",
    "            \"tune\": 100,\n",
    "            \"cores\": 4,\n",
    "            \"chains\": 4,\n",
    "            \"compute_convergence_checks\": False\n",
    "        }\n",
    "\n",
    "# initialize bart\n",
    "bart_model = bmb.BartSurvModel(model_config=model_dict, sampler_config=sampler_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce708f5-f96e-4319-888f-943836f9ee1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below the model is fit to the data and posterior predictions of the \"mu\" parameter (this is our probability risk of event occuring at time) is drawn for each obs at each time point. \n",
    "\n",
    "When the model is fit in the same instance that posterior predictions are made we can use the class method sample_posterior_predictive. \n",
    "\n",
    "When a trained model is loaded from a prior instance the bart_predict function is used. This comes from an instability of saving the model object internal to the pymc-bart implementation.\n",
    "\n",
    "The class method save will save both the trace of the trained model (idata) and the trained tree structure. In the survival setting only the tree structure is required for future predictions.\n",
    "\n",
    "The functions get_prob and get_survival return the predicted risk probabilites and computed survival estimates for each observations for each of the time points. \n",
    "- The returned estimates are point-estimates for each patient and stratified credible intervals and point estimates can be drawn from the predictions. These functions will be adjusted to return the raw draws from posterior to compute quantile estimates as well in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a583e585-f0a5-4e88-af14-f900ff4a6510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "bart_model.fit(coh_y, coh_x, coh_w, coh_coords)\n",
    "# sample posterior\n",
    "post = bart_model.sample_posterior_predictive(x_tst, tst_coords, extend_idata=True)\n",
    "# get posterior data\n",
    "prob = bmb.get_prob(post)\n",
    "sv = bmb.get_survival(post)\n",
    "\n",
    "bart_model.save(idata_name=\"/tmp/test_idata1.pkl\", all_tree_name=\"/tmp/test_tree1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b84316c-1ca7-4c42-8358-101c0110309f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save idata and tree\n",
    "bart_model.save(idata_name=\"/tmp/test_idata1.pkl\", all_tree_name=\"/tmp/test_tree1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb2d22f-59c9-4685-beb3-ea250fa4dc71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A Variable importance can be computed from the trained model. The pymc bart package provides an algortihm that computes additives importance of ranked variable as shown in the blocked out code below. This methods takes time to complete and does not provide a large benefit in comparision to the more naive measure of variable importance as shown in the second code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fef5c7f8-5fcf-4b22-b450-9868152cbf89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# not needed, used simple var-importance\n",
    "# pmb.plot_variable_importance(\n",
    "#     idata=bart_model.idata,\n",
    "#     bartrv=bart_model.model.f,\n",
    "#     X=x_tst,\n",
    "#     samples=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc881eb-d394-42f4-b049-5b85aabe5953",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Naive variable importances shows variable 0 is the most important (the time), followed by several other variables that are included in the data generating process.\n",
    "\n",
    "The value associated with each variable is the mean inclusion frequency over all of the draws. Unfortunately, this method is not quantitatively robust and the values provide no quantitative purpose beyond relative rank in assessment in the trained model. \n",
    "\n",
    "It is useful as a overview of importance similar to variable importance measures returned from Random Forest methods and can direct the follow-up with the Marginal Dependence Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646dc38e-dc1b-4fb8-8d86-f3e2dd6f5376",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# naive variable importance\n",
    "vars_tree = bart_model.idata.sample_stats.variable_inclusion.values.reshape(400,-1)\n",
    "vmean = vars_tree.mean(0)\n",
    "var_dict = dict(zip(np.argsort(-vmean), -np.sort(-vmean)))\n",
    "var_dict\n",
    "# save this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8a037c-787c-4cb5-b205-0ab775a91fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cox Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b670a641-cafa-4f27-8247-4388d6dc88be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Train a cox model for comparison purposes.\n",
    "\n",
    "While the exp(coef) returned do not have associated confidence intervals, the point estimate can be used as an approximate comparison of variable importance between cox and bart models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b313219f-729f-43c1-ae62-c38a512a1480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cox model\n",
    "cph_coef, cph_sv, cph_chz = ut.get_cph(y_sk, x_mat, x_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45316fe8-f0f4-4334-9f36-3f95e8409af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cph_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63382fba-c913-4995-9645-665e0a045243",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59518ab5-c543-42d1-b30e-685a72919e17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Compare the estimated survival curves of the two models to the kpm estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a054708-8be7-46e9-8b74-a145d441df73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ut.quick_kpm_plot(y_sk, msk=x_mat[:,0]==1, cph_sv=cph_sv, sv=sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1ce30d-1a1d-4504-a3de-f034ea8360ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get evaluations metrics including the cindex and brier score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc6ec0f-e6b5-457f-bcfd-3add48d0c592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bart_met = ut.get_sv_metrics(sv, prob, y_sk_coh)\n",
    "cph_met = ut.get_sv_metrics(cph_sv, cph_chz, y_sk)\n",
    "\n",
    "# log these\n",
    "print(bart_met)\n",
    "print(cph_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f785740-bcad-433a-8fda-f358ceb0a1f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1acda01a-8cfd-41e9-aa2e-b93de93c3f15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To preform out of sample evaluations use the generated test dataset. \n",
    "\n",
    "The same data preparations can be used as above.\n",
    "\n",
    "The bart_model.sample_posterior_predictive can be used to generate predicitons if the testing is be completed in the same instance of the model.fit run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbf83d99-1caf-48b5-99e5-f24c916821bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "t_t_event2 = bmb.get_time_transform(t_t_event, time_scale=time_scale)\n",
    "t_y_sk = bmb.get_y_sklearn(t_status, t_t_event2)\n",
    "t_y_sk_coh, t_x_sk_coh, t_w_coh= bmb.get_case_cohort(t_y_sk, t_x_mat, 1)\n",
    "t_x_tst, t_tst_coords = bmb.get_posterior_test(np.unique(t_y_sk_coh[\"Survival_in_days\"]), t_x_sk_coh)\n",
    "\n",
    "t_post = bart_model.sample_posterior_predictive(t_x_tst, t_tst_coords, extend_idata=False)\n",
    "t_prob = bmb.get_prob(t_post)\n",
    "t_sv = bmb.get_survival(t_post)\n",
    "\n",
    "# cox\n",
    "t_cph_coef, t_cph_sv, t_cph_chz = ut.get_cph(y_sk, x_mat, t_x_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba3bb268-272d-49a5-ad6b-b1cf6b97c499",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test dataset evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35bf1ed9-17b1-45d2-89ee-d93ea208d7ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ut.quick_kpm_plot(t_y_sk, t_x_mat[:,0]==1, t_cph_sv, t_sv)\n",
    "# save this\n",
    "t_bart_met = ut.get_sv_metrics(t_sv, t_prob, t_y_sk_coh, y_sk_coh)\n",
    "t_cph_met = ut.get_sv_metrics(t_cph_sv, t_cph_chz, t_y_sk, y_sk)\n",
    "\n",
    "print(t_bart_met)\n",
    "print(t_cph_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46a0f219-b85a-4aa0-856f-14c2ba15d62a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# True Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae70819-fed4-488e-b114-1aa55dda9cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If working with the simulated data, a evaluation can be completed with the true sv/hz estimates. The primary values estimated include rmse and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "053f815e-8e80-4c0c-9b04-6077704f7b5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp_times = [0,1,2,3,4,5]\n",
    "b_true_eval = ut.get_true_rmse_bias(\n",
    "    true_scale[\"sv_true\"][:,1:], \n",
    "    sv,\n",
    "    tmp_times\n",
    "    )\n",
    "\n",
    "cph_true_eval = ut.get_true_rmse_bias(\n",
    "    true_scale[\"sv_true\"][:,1:], \n",
    "    cph_sv,\n",
    "    tmp_times\n",
    "    )\n",
    "\n",
    "\n",
    "t_b_true_eval = ut.get_true_rmse_bias(\n",
    "    t_true_scale[\"sv_true\"][:,1:], \n",
    "    t_sv,\n",
    "    tmp_times\n",
    "    )\n",
    "\n",
    "t_cph_true_eval = ut.get_true_rmse_bias(\n",
    "    t_true_scale[\"sv_true\"][:,1:], \n",
    "    t_cph_sv,\n",
    "    tmp_times\n",
    "    )\n",
    "\n",
    "print(b_true_eval)\n",
    "print(cph_true_eval)\n",
    "print(t_b_true_eval)\n",
    "print(t_cph_true_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c14f066-2477-4923-ab22-7660d489893e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A sample comparison of predictions from the different models in comparison to the true sv can also demonstrated through sv plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00427864-47c8-482d-bd32-635d0d719dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp_times = [1,2,3,4,5,6]\n",
    "\n",
    "for i in np.arange(0,10):\n",
    "    if i == 0:\n",
    "        plt.plot(tmp_times, t_true_scale[\"sv_true\"][i,1:], color= \"red\", alpha= 0.2, label=\"exp\")\n",
    "        plt.plot(tmp_times, t_sv[i,:], color= \"blue\", alpha= 0.2, label=\"pred_b\")\n",
    "        plt.plot(tmp_times, t_cph_sv[i,:], color= \"green\", alpha= 0.2, label = \"pred_cph\")\n",
    "    else:\n",
    "        plt.plot(tmp_times, t_true_scale[\"sv_true\"][i,1:], color= \"red\", alpha= 0.2)\n",
    "        plt.plot(tmp_times, t_sv[i,:], color= \"blue\", alpha= 0.2)\n",
    "        plt.plot(tmp_times, t_cph_sv[i,:], color= \"green\", alpha= 0.2)\n",
    "\n",
    "plt.legend()\n",
    "# t_true_scale\n",
    "# t_sv[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086dff3a-d2b4-49ee-a545-a3c8a66675c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The stratified estimates by the main variable can be plotted to compare how well the model does at computing the sub-group sv means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e8b0e21-e3b8-4a96-a0c5-b38964cac58a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "msk = t_x_mat[:,0] == 1\n",
    "\n",
    "m1 = t_true_scale[\"sv_true\"][msk,1:].mean(axis=0)\n",
    "m1_ = t_true_scale[\"sv_true\"][~msk,1:].mean(axis=0)\n",
    "m2 = t_sv[msk,:].mean(axis=0)\n",
    "m2_ = t_sv[~msk,:].mean(axis=0)\n",
    "m3 = t_cph_sv[msk,:].mean(axis=0)\n",
    "m3_ = t_cph_sv[~msk,:].mean(axis=0)\n",
    "\n",
    "\n",
    "tmp_times = [1,2,3,4,5,6]\n",
    "plt.plot(tmp_times, m1, color = \"red\", label = \"exp\")\n",
    "plt.plot(tmp_times, m1_, color= \"pink\", label = \"exp\")\n",
    "plt.plot(tmp_times, m2, color = \"blue\", label=\"bart\")\n",
    "plt.plot(tmp_times, m2_, color = \"lightblue\", label= \"bart\")\n",
    "plt.plot(tmp_times, m3, color = \"green\", label= \"cph\")\n",
    "plt.plot(tmp_times, m3_, color = \"lightgreen\", label = \"cph\")\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed414aa4-697c-4891-b295-0edc56d263a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PDP Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9cc3f2-58b2-440a-97d7-89eb9c32fb34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PDP evaluations allow estimations of marginal effects of a variable. This is part of the primary outcome of the proposed study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62f3dda2-c7ba-45b4-80cd-9491f4f09922",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the databricks workflows this process will be seperated from the model training, to allow for specific pdps to tested. \n",
    "\n",
    "Doing this in a new compute instance requires predictions to be drawn with a different method.\n",
    "\n",
    "1. Instantiate a new surv_bart model and load the saved trace and tree structure.\n",
    "2. Collect the training data loaded with the model. \n",
    "3. get_pdp is a function that generates a pdp testing dataset. One or two variables can be selected for the pdp and a subsample of the training data can be used if the data is large.\n",
    "4. use get_posterior_test to create the long-form dataset for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d0c714-e003-42bf-a932-91a8b87e8f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bart_m2 = bmb.BartSurvModel(model_config=model_dict).load(\"test_idata1.pkl\", \"test_tree1.pkl\")\n",
    "x2 = bart_m2.X[bart_m2.X[:,0]==1][:,1:]\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [0], values = [[0,1]], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfcdaa9b-5938-418a-ba32-3ad9a735bf76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. bart_predict - returns posterior draws of the risk probability, similar to get_posterior_predictions above\n",
    "2. get_survival/get_prob to return patientwise point estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b494aaa-63dc-4ce1-85c7-9f9c2d7d5445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "155d96a4-7e1e-4689-8565-fbd5f926c977",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For comparison agains the true in a simulated scenario the sim_surv function can be used with the same parameters used in the initial instance, except the x_mat dataset is replaced with the pdp_sk dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be9d4938-7f1f-44a6-8747-00638dfc7441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdp_scale = ut.sim_surv(pdp_sk, LAMBDA, ALPHA_F, eos, time_scale=time_scale, return_full=False, true_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5f5871-bf58-417e-a10b-3e252905eca2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Plot the expected (true) vs predicted values for each patient. The will ideally follow the 1:1 axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0051ff04-a4ee-4eec-8b5f-f520a1a71f10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# true plot\n",
    "plt.plot(pdp_scale[\"sv_true\"][:,1], pdp_sv[:,0], \"bo\", alpha=0.02)\n",
    "plt.plot(pdp_scale[\"sv_true\"][:,2], pdp_sv[:,1], \"ro\", alpha=0.02)\n",
    "plt.plot(pdp_scale[\"sv_true\"][:,3], pdp_sv[:,2], \"go\", alpha=0.02)\n",
    "plt.plot(pdp_scale[\"sv_true\"][:,4], pdp_sv[:,3], \"ko\", alpha=0.02)\n",
    "plt.plot(pdp_scale[\"sv_true\"][:,5], pdp_sv[:,4], \"mo\", alpha=0.02)\n",
    "plt.plot(pdp_scale[\"sv_true\"][:,6], pdp_sv[:,5], \"wo\", alpha=0.02)\n",
    "\n",
    "plt.grid(visible=True)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"pred\")\n",
    "plt.xlabel(\"exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62198020-c3c1-4811-9c19-499b13553af7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Plot the pdp comparison of expected pdp and predicted pdp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c7d18d-8b98-46d2-ad55-486961ba6bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exp0 = pdp_scale[\"sv_true\"][pdp_idx[\"coord\"]==0]\n",
    "exp1 = pdp_scale[\"sv_true\"][pdp_idx[\"coord\"]==1]\n",
    "edm = (exp1-exp0).mean(0)\n",
    "edq = edm - np.quantile((exp1-exp0), [0.025, 0.975], axis=0)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = edm[1:], yerr = np.abs(edq[:,1:]), alpha=0.2, label = \"exp\")\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0987ac5f-df77-49bb-b420-7239960ff796",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hazard ratios/Risk ratios can also be computed from the pdp predictions using the risk probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6956d191-1342-4030-a1c6-e55b89f2674f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hr\n",
    "hr_true = (pdp_scale[\"hz_true\"][pdp_idx[\"coord\"]==1]/pdp_scale[\"hz_true\"][pdp_idx[\"coord\"]==0])\n",
    "hr_true_m = hr_true.mean(0)\n",
    "hr_true_q = np.quantile(hr_true, [0.025, 0.975], 0)\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d654580-54ab-478d-bc08-1e38289487a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There may be some notable differences in this ratio since the true would be more equivalent to a HR and the expected is a Risk Ratio and some difference exist between these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3e5e13-95b1-4d44-967b-3520459e18d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(hr_true_m)\n",
    "print(hr_true_q)\n",
    "print(hr_true_m[1:].mean())\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9a8732-f60a-4177-baf6-aa95d9fcf1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## General PDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5c4c51-a45f-4deb-baf6-8bac7d83c0a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Outside of the simulation scenario, each variable can be evaluated with a pdp to derive estimated marginal effect of the variable.\n",
    "\n",
    "The magnitude of estimated effects should follow the rank of the variable importance given above.\n",
    "\n",
    "NOTE: Sometimes we refer to the variable 0 as the time and othertimes we refer to it as the first variable of the covariate matrix. When generating the PDPs the first variables of covariate matrix is index 0 and once the long form predicton matrix is generated the index 0 variable is now time, moving the first variable to index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29638f90-fa26-4a76-a74a-db4a64836bda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 0\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5255860-c6b4-434c-9542-f6d936b4c97f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 1\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b37b67-88d3-4327-88b7-dc19355d2051",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 2\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5ee94d0-dd77-4a16-a366-adda080b5b5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 3\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396e38cb-079a-4da8-8137-d957febfb8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 4\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07cd0804-d5fa-4fe8-b237-5d56a33a4e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 5\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "791845a4-2cd6-4c7a-af06-581979ee0c63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 6\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f4c3e4-fd61-40b9-9de9-6a1be90b2f6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 7\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72c35b1-90c1-4c04-b189-01d2110d71dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 8\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0217415-b47d-4610-a055-97d117ba23f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var_pdp = 9\n",
    "v2q = np.quantile(x2[:,var_pdp], [0.25,0.75])\n",
    "pdp_sk, pdp_idx = bmb.get_pdp(x2, var_col = [var_pdp], values = [v2q], sample_n=1000)\n",
    "pdp_x, pdp_coords = bmb.get_posterior_test(bart_m2.uniq_times, pdp_sk)\n",
    "pdp_post = bart_m2.bart_predict(pdp_x, pdp_coords)\n",
    "pdp_sv = bmb.get_survival(pdp_post)\n",
    "pdp_prob = bmb.get_prob(pdp_post)\n",
    "\n",
    "pred0 = pdp_sv[pdp_idx[\"coord\"]==0]\n",
    "pred1 = pdp_sv[pdp_idx[\"coord\"]==1]\n",
    "pdm = (pred1-pred0).mean(0)\n",
    "pdq = pdm - np.quantile((pred1-pred0), [0.025, 0.975], axis=0)\n",
    "\n",
    "plt.errorbar([1,2,3,4,5,6], y = pdm, yerr = np.abs(pdq), alpha=0.2, label = \"pred\")\n",
    "plt.legend()\n",
    "\n",
    "rr_pred = (pdp_prob[pdp_idx[\"coord\"]==1]/pdp_prob[pdp_idx[\"coord\"]==0])\n",
    "rr_pred_m = rr_pred.mean(0)\n",
    "rr_pred_q = np.quantile(rr_pred, [0.025,0.975], 0)\n",
    "\n",
    "print(rr_pred_m)\n",
    "print(rr_pred_q)\n",
    "print(rr_pred_m.mean())\n",
    "print(rr_pred_q.mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d624750-cebe-4b22-aa53-a9c8c6afbb33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Comparisons of the estimated Risk Ratio can be compared to CPH HR that are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bff39a78-67dd-4cd4-8da6-a6edd0ec403a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cph_coef"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sv_bart_run1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
